/**
 * AudioNarrationEngine - Pre-generated audio playback with word-level highlighting
 * 
 * Works with audio files generated by generate-audio.js using OpenAI TTS + Whisper
 * for precise word timestamps.
 * 
 * Usage:
 *   const narrator = new AudioNarrationEngine('audio');
 *   await narrator.playStep('service-story', 0);
 * 
 * Directory Structure:
 *   audio/
 *     manifest.json
 *     {voiceId}/
 *       {storyId}/
 *         step-0.mp3
 *         step-1.mp3
 *         ...
 */

class AudioNarrationEngine {
  constructor(audioBasePath = 'audio') {
    this.audioBasePath = audioBasePath;
    this.isMuted = false;
    this.isSpeaking = false;
    this.isPaused = false;
    this.currentAudio = null;
    this.currentTimestamps = null;
    this.highlightInterval = null;
    this.onSpeakingChange = null;
    this.onWordHighlight = null; // Callback: (wordIndex, word) => void
    this.resolvePromise = null;
    this.manifest = null;
    this.currentVoice = 'ballad'; // Default voice
    
    this.loadManifest();
  }

  async loadManifest() {
    try {
      const response = await fetch(`${this.audioBasePath}/manifest.json`);
      this.manifest = await response.json();
      this.currentVoice = this.manifest.defaultVoice || 'ballad';
      console.log('Audio manifest loaded. Voices:', Object.keys(this.manifest.voices || {}));
    } catch (e) {
      console.warn('Could not load audio manifest:', e);
    }
  }

  getVoices() {
    if (!this.manifest || !this.manifest.voices) return [];
    return Object.entries(this.manifest.voices).map(([key, config]) => ({
      id: key,
      name: config.name,
      label: config.label
    }));
  }

  setVoice(voiceId) {
    if (this.manifest && this.manifest[voiceId]) {
      this.currentVoice = voiceId;
      console.log('Voice changed to:', voiceId);
    }
  }

  getTimestamps(storyId, stepIndex) {
    if (!this.manifest || !this.manifest[this.currentVoice]) return null;
    const voiceData = this.manifest[this.currentVoice];
    if (!voiceData[storyId]) return null;
    return voiceData[storyId][stepIndex];
  }

  setMuted(muted) {
    this.isMuted = muted;
    if (muted && this.currentAudio) {
      this.currentAudio.volume = 0;
    } else if (this.currentAudio) {
      this.currentAudio.volume = 1;
    }
  }

  stop() {
    if (this.currentAudio) {
      this.currentAudio.pause();
      this.currentAudio.currentTime = 0;
      this.currentAudio = null;
    }
    if (this.highlightInterval) {
      cancelAnimationFrame(this.highlightInterval);
      this.highlightInterval = null;
    }
    this.isSpeaking = false;
    this.isPaused = false;
    this.currentTimestamps = null;
    if (this.onSpeakingChange) this.onSpeakingChange(false);
    if (this.resolvePromise) {
      this.resolvePromise();
      this.resolvePromise = null;
    }
  }

  pause() {
    if (this.currentAudio && this.isSpeaking && !this.isPaused) {
      this.currentAudio.pause();
      this.isPaused = true;
    }
  }

  resume() {
    if (this.currentAudio && this.isPaused) {
      this.currentAudio.play();
      this.isPaused = false;
    }
  }

  get paused() {
    return this.isPaused;
  }

  /**
   * Play pre-generated audio with word highlighting
   * @param {string} storyId - The story identifier (e.g., 'service-story')
   * @param {number} stepIndex - The step index within the story
   * @returns {Promise} Resolves when audio finishes or is stopped
   */
  playStep(storyId, stepIndex) {
    return new Promise((resolve) => {
      const timestamps = this.getTimestamps(storyId, stepIndex);
      
      if (!timestamps || this.isMuted) {
        resolve();
        return;
      }

      const audioPath = `${this.audioBasePath}/${this.currentVoice}/${storyId}/step-${stepIndex}.mp3`;
      
      this.stop(); // Stop any current playback
      this.resolvePromise = resolve;
      this.currentTimestamps = timestamps;

      const audio = new Audio(audioPath);
      this.currentAudio = audio;
      audio.volume = this.isMuted ? 0 : 1;

      let lastHighlightedIndex = -1;

      // Word highlighting loop using requestAnimationFrame
      const highlightLoop = () => {
        if (!this.currentAudio || this.isPaused) {
          this.highlightInterval = requestAnimationFrame(highlightLoop);
          return;
        }

        const currentTime = audio.currentTime;
        const words = timestamps.words;

        // Find the current word based on timestamp
        let currentWordIndex = -1;
        for (let i = 0; i < words.length; i++) {
          if (currentTime >= words[i].start && currentTime < words[i].end + 0.1) {
            currentWordIndex = i;
            break;
          }
          // If we're past this word but before the next, still highlight it
          if (currentTime >= words[i].start && (i === words.length - 1 || currentTime < words[i + 1].start)) {
            currentWordIndex = i;
            break;
          }
        }

        // Trigger highlight callback if word changed
        if (currentWordIndex !== lastHighlightedIndex && currentWordIndex >= 0) {
          lastHighlightedIndex = currentWordIndex;
          if (this.onWordHighlight) {
            this.onWordHighlight(currentWordIndex, words[currentWordIndex].word);
          }
        }

        if (this.isSpeaking && !audio.ended) {
          this.highlightInterval = requestAnimationFrame(highlightLoop);
        }
      };

      audio.onplay = () => {
        this.isSpeaking = true;
        this.isPaused = false;
        if (this.onSpeakingChange) this.onSpeakingChange(true);
        highlightLoop();
      };

      audio.onpause = () => {
        if (!audio.ended) {
          this.isPaused = true;
        }
      };

      audio.onended = () => {
        this.isSpeaking = false;
        this.isPaused = false;
        if (this.highlightInterval) {
          cancelAnimationFrame(this.highlightInterval);
          this.highlightInterval = null;
        }
        if (this.onSpeakingChange) this.onSpeakingChange(false);
        
        // Final highlight - mark all words as spoken
        if (this.onWordHighlight) {
          this.onWordHighlight(timestamps.words.length, null); // Signal completion
        }
        
        this.resolvePromise = null;
        resolve();
      };

      audio.onerror = (e) => {
        console.warn('Audio error:', e);
        this.isSpeaking = false;
        this.isPaused = false;
        if (this.onSpeakingChange) this.onSpeakingChange(false);
        this.resolvePromise = null;
        resolve();
      };

      audio.play().catch(e => {
        console.warn('Audio play failed:', e);
        resolve();
      });
    });
  }

  /**
   * Check if audio is available for a story
   * @param {string} storyId 
   * @returns {boolean}
   */
  hasAudioForStory(storyId) {
    return this.manifest && 
           this.manifest[this.currentVoice] && 
           this.manifest[this.currentVoice][storyId];
  }

  /**
   * Get total number of steps with audio for a story
   * @param {string} storyId 
   * @returns {number}
   */
  getStepCount(storyId) {
    if (!this.hasAudioForStory(storyId)) return 0;
    return Object.keys(this.manifest[this.currentVoice][storyId]).length;
  }
}

// Export for use as module or global
if (typeof module !== 'undefined' && module.exports) {
  module.exports = AudioNarrationEngine;
} else if (typeof window !== 'undefined') {
  window.AudioNarrationEngine = AudioNarrationEngine;
}
